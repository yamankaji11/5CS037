{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "81wYgbMdsFGu"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "IuyfeY7dU4lh"
      },
      "outputs": [],
      "source": [
        "def logistic_function(x):\n",
        "    \"\"\"\n",
        "    Computes the logistic function applied to any value of x.\n",
        "    Arguments:\n",
        "    x: scalar or numpy array of any size.\n",
        "    Returns:\n",
        "    y: logistic function applied to x.\n",
        "    \"\"\"\n",
        "    y = 1 / (1 + np.exp(-x))\n",
        "    return y\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_logistic_function():\n",
        "    \"\"\"\n",
        "    Test cases for the logistic_function.\n",
        "    \"\"\"\n",
        "\n",
        "    # Test with scalar input\n",
        "    x_scalar = 0\n",
        "    expected_output_scalar = round(1 / (1 + np.exp(0)), 3)\n",
        "    assert round(logistic_function(x_scalar), 3) == expected_output_scalar, \"Test failed for scalar input\"\n",
        "\n",
        "    # Test with positive scalar input\n",
        "    x_pos = 2\n",
        "    expected_output_pos = round(1 / (1 + np.exp(-2)), 3)\n",
        "    assert round(logistic_function(x_pos), 3) == expected_output_pos, \"Test failed for positive scalar input\"\n",
        "\n",
        "    # Test with negative scalar input\n",
        "    x_neg = -3\n",
        "    expected_output_neg = round(1 / (1 + np.exp(3)), 3)\n",
        "    assert round(logistic_function(x_neg), 3) == expected_output_neg, \"Test failed for negative scalar input\"\n",
        "\n",
        "    # Test with numpy array input\n",
        "    x_array = np.array([0, 2, -3])\n",
        "    expected_output_array = np.array([0.5, 0.881, 0.047])\n",
        "    assert np.all(np.round(logistic_function(x_array), 3) == expected_output_array), \"Test failed for numpy array input\"\n",
        "\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "# Run the test case\n",
        "test_logistic_function()\n"
      ],
      "metadata": {
        "id": "jEC7jkxdiNlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92679bde-d7c4-4939-8ccc-75c4c0504b43"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Computes log loss for true target value y ={0 or 1} and predicted target value yâ€™ inbetween {0-1}.\n",
        "    Arguments:\n",
        "    y_true (scalar): true target value {0 or 1}.\n",
        "    y_pred (scalar): predicted taget value {0-1}.\n",
        "    Returns:\n",
        "    loss (float): loss/error value\n",
        "    \"\"\"\n",
        "    # Ensure y_pred is clipped to avoid log(0)\n",
        "    y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
        "\n",
        "    loss = -(y_true * np.log(y_pred)) - ((1 - y_true) * np.log(1 - y_pred))\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "-uC1fYASlgLI"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true, y_pred = 0, 0.1\n",
        "print(f'log loss({y_true}, {y_pred}) ==> {log_loss(y_true, y_pred)}')\n",
        "\n",
        "print(\"+++++++++++++--------------------------++++++++++++++++++++++++\")\n",
        "\n",
        "y_true, y_pred = 1, 0.9\n",
        "print(f'log loss({y_true}, {y_pred}) ==> {log_loss(y_true, y_pred)}')\n"
      ],
      "metadata": {
        "id": "VzyhbcvzpAyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef71b419-aa4e-43d8-ccd0-239da884c09f"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log loss(0, 0.1) ==> 0.10536051565782628\n",
            "+++++++++++++--------------------------++++++++++++++++++++++++\n",
            "log loss(1, 0.9) ==> 0.10536051565782628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_log_loss():\n",
        "  \"\"\"\n",
        "  Test cases for the log_loss function.\n",
        "  \"\"\"\n",
        "  import numpy as np\n",
        "  # Test case 1: Perfect prediction (y_true = 1, y_pred = 1)\n",
        "  y_true = 1\n",
        "  y_pred = 1\n",
        "  expected_loss = 0.0 # Log loss is 0 for perfect prediction\n",
        "  assert np.isclose(log_loss(y_true, y_pred), expected_loss), \"Test failed for perfect prediction (y_true=1, y_pred=1)\"\n",
        "  # Test case 2: Perfect prediction (y_true = 0, y_pred = 0)\n",
        "  y_true = 0\n",
        "  y_pred = 0\n",
        "  expected_loss = 0.0 # Log loss is 0 for perfect prediction\n",
        "  assert np.isclose(log_loss(y_true, y_pred), expected_loss), \"Test failed for perfect prediction (y_true=0, y_pred=0)\"\n",
        "  # Test case 3: Incorrect prediction (y_true = 1, y_pred = 0)\n",
        "  y_true = 1\n",
        "  y_pred = 0\n",
        "  try:\n",
        "    log_loss(y_true, y_pred) # This should raise an error due to log(0)\n",
        "  except ValueError:\n",
        "    pass # Test passed if ValueError is raised for log(0)\n",
        "  # Test case 4: Incorrect prediction (y_true = 0, y_pred = 1)\n",
        "  y_true = 0\n",
        "  y_pred = 1\n",
        "  try:\n",
        "    log_loss(y_true, y_pred) # This should raise an error due to log(0)\n",
        "  except ValueError:\n",
        "    pass # Test passed if ValueError is raised for log(0)\n",
        "  # Test case 5: Partially correct prediction\n",
        "  y_true = 1\n",
        "  y_pred = 0.8\n",
        "  expected_loss = -(1 * np.log(0.8)) - (0 * np.log(0.2)) # ~0.2231\n",
        "  assert np.isclose(log_loss(y_true, y_pred), expected_loss, atol=1e-6), \"Test failed for partially correct prediction (y_true=1, y_pred=0.8)\"\n",
        "  y_true = 0\n",
        "  y_pred = 0.2\n",
        "  expected_loss = -(0 * np.log(0.2)) - (1 * np.log(0.8)) # ~0.2231\n",
        "  assert np.isclose(log_loss(y_true, y_pred), expected_loss, atol=1e-6), \"Test failed for partially correct prediction (y_true=0, y_pred=0.2)\"\n",
        "  print(\"All tests passed!\")\n",
        "# Run the test case\n",
        "test_log_loss()"
      ],
      "metadata": {
        "id": "fnlnRHr9peTk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e28f46cc-5c7d-4c4e-8e54-2e9f1a382531"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_function(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Computes log loss for inputs true value (0 or 1) and predicted value (between 0 and 1)\n",
        "\n",
        "    Args:\n",
        "    y_true (array_like, shape (n,)): array of true values (0 or 1)\n",
        "    y_pred (array_like, shape (n,)): array of predicted values (probability of y_pred being 1)\n",
        "\n",
        "    Returns:\n",
        "    cost (float): nonnegative cost corresponding to y_true and y_pred\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    # Ensure y_pred is clipped to avoid log(0)\n",
        "    y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
        "\n",
        "    # Number of samples\n",
        "    n = len(y_true)\n",
        "\n",
        "    # Compute loss for each sample\n",
        "    loss_vec = - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "    # Average loss across all samples\n",
        "    cost = np.sum(loss_vec) / n\n",
        "\n",
        "    return cost\n"
      ],
      "metadata": {
        "id": "D3iN4Bg5p5l6"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_cost_function():\n",
        "  # Test case 1: Simple example with known expected cost\n",
        "  y_true = np.array([1, 0, 1])\n",
        "  y_pred = np.array([0.9, 0.1, 0.8])\n",
        "  # Expected output: Manually calculate cost for these values\n",
        "  # log_loss(y_true, y_pred) for each example\n",
        "  expected_cost = (-(1 * np.log(0.9)) - (1 - 1) * np.log(1 - 0.9) +\n",
        "  -(0 * np.log(0.1)) - (1 - 0) * np.log(1 - 0.1) +\n",
        "  -(1 * np.log(0.8)) - (1 - 1) * np.log(1 - 0.8)) / 3\n",
        "\n",
        "  # Call the cost_function to get the result\n",
        "  result = cost_function(y_true, y_pred)\n",
        "  # Assert that the result is close to the expected cost with a tolerance of 1e-6\n",
        "  assert np.isclose(result, expected_cost, atol=1e-6), f\"Test failed: {result} != {expected_cost}\"\n",
        "  print(\"Test passed for simple case!\")\n",
        "\n",
        "# Run the test case\n",
        "test_cost_function()"
      ],
      "metadata": {
        "id": "hEOsIw6DqQMe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caa8ca8c-d68e-4083-98b2-10944a1e47f4"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed for simple case!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def costfunction_logreg(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the cost function, given data and model parameters.\n",
        "    Args:\n",
        "    X (ndarray, shape (m,n)): data on features, m observations with n features.\n",
        "    y (array_like, shape (m,)): array of true values of target (0 or 1).\n",
        "    w (array_like, shape (n,)): weight parameters of the model.\n",
        "    b (float): bias parameter of the model.\n",
        "    Returns:\n",
        "    cost (float): nonnegative cost corresponding to y and y_pred.\n",
        "    \"\"\"\n",
        "\n",
        "    n, d = X.shape\n",
        "\n",
        "    assert len(y) == n, \"Number of feature observations and number of target observations do not match.\"\n",
        "    assert len(w) == d, \"Number of features and number of weight parameters do not match.\"\n",
        "\n",
        "    # Compute z using np.dot\n",
        "    z = np.dot(X, w) + b                  # Your Code Here\n",
        "\n",
        "    # Compute predictions using logistic function (sigmoid)\n",
        "    y_pred = logistic_function(z)         # Your Code Here\n",
        "\n",
        "    # Compute the cost using the cost function\n",
        "    cost = cost_function(y, y_pred)\n",
        "\n",
        "    return cost\n"
      ],
      "metadata": {
        "id": "hr-i0_B8kbGc"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[10, 20], [-10, 10]])\n",
        "y = np.array([1, 0])\n",
        "w = np.array([0.5, 1.5])\n",
        "b = 1\n",
        "\n",
        "print(f\"cost for logistic regression(X = {X}, y = {y}, w = {w}, b = {b}) = {costfunction_logreg(X, y, w, b)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UAzNiyCkcG9",
        "outputId": "e53c6886-acd8-488a-a72d-907746aacaeb"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost for logistic regression(X = [[ 10  20]\n",
            " [-10  10]], y = [1 0], w = [0.5 1.5], b = 1) = 5.500008350834906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes gradients of the cost function with respect to model parameters.\n",
        "    Args:\n",
        "    X (ndarray, shape (n,d)): Input data, n observations with d features\n",
        "    y (array_like, shape (n,)): True labels (0 or 1)\n",
        "    w (array_like, shape (d,)): Weight parameters of the model\n",
        "    b (float): Bias parameter of the model\n",
        "    Returns:\n",
        "    grad_w (array_like, shape (d,)): Gradients of the cost function with respect to the weight parameters\n",
        "    grad_b (float): Gradient of the cost function with respect to the bias parameter\n",
        "    \"\"\"\n",
        "\n",
        "    n, d = X.shape\n",
        "\n",
        "    assert len(y) == n, f\"Expected y to have {n} elements, but got {len(y)}\"\n",
        "    assert len(w) == d, f\"Expected w to have {d} elements, but got {len(w)}\"\n",
        "\n",
        "    # Compute predictions using logistic function (sigmoid)\n",
        "    y_pred = logistic_function(np.dot(X, w) + b)   # Your Code Here\n",
        "\n",
        "    # Compute gradients\n",
        "    grad_w = -(1/n) * np.dot(X.T, (y - y_pred))    # Your Code Here\n",
        "    grad_b = -(1/n) * np.sum(y - y_pred)           # Your Code Here\n",
        "\n",
        "    return grad_w, grad_b\n"
      ],
      "metadata": {
        "id": "d7AsVANskjFm"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[10, 20], [-10, 10]])\n",
        "y = np.array([1, 0])\n",
        "w = np.array([0.5, 1.5])\n",
        "b = 1\n",
        "\n",
        "try:\n",
        "    grad_w, grad_b = compute_gradient(X, y, w, b)\n",
        "    print(\"Gradients computed successfully.\")\n",
        "    print(f\"grad_w: {grad_w}\")\n",
        "    print(f\"grad_b: {grad_b}\")\n",
        "except AssertionError as e:\n",
        "    print(f\"Assertion error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXCleNlzkk7Q",
        "outputId": "ca8b265a-a8a2-4b4f-c29d-97b232f4fb96"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients computed successfully.\n",
            "grad_w: [-4.99991649  4.99991649]\n",
            "grad_b: 0.4999916492890759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w, b, alpha, n_iter, show_cost=False, show_params=True):\n",
        "    \"\"\"\n",
        "    Implements batch gradient descent to optimize logistic regression parameters.\n",
        "    Args:\n",
        "    X (ndarray, shape (n,d)): Data on features, n observations with d features\n",
        "    y (array_like, shape (n,)): True values of target (0 or 1)\n",
        "    w (array_like, shape (d,)): Initial weight parameters\n",
        "    b (float): Initial bias parameter\n",
        "    alpha (float): Learning rate\n",
        "    n_iter (int): Number of iterations\n",
        "    show_cost (bool): If True, displays cost every 100 iterations\n",
        "    show_params (bool): If True, displays parameters every 100 iterations\n",
        "    Returns:\n",
        "    w (array_like, shape (d,)): Optimized weight parameters\n",
        "    b (float): Optimized bias parameter\n",
        "    cost_history (list): List of cost values over iterations\n",
        "    params_history (list): List of parameters (w, b) over iterations\n",
        "    \"\"\"\n",
        "\n",
        "    n, d = X.shape\n",
        "\n",
        "    assert len(y) == n, \"Number of observations in X and y do not match\"\n",
        "    assert len(w) == d, \"Number of features in X and w do not match\"\n",
        "\n",
        "    cost_history = []\n",
        "    params_history = []\n",
        "\n",
        "    for i in range(n_iter):\n",
        "\n",
        "        # Compute gradients\n",
        "        grad_w, grad_b = compute_gradient(X, y, w, b)   # Your Code Here\n",
        "\n",
        "        # Update weights and bias\n",
        "        w -= alpha * grad_w                             # Your Code Here\n",
        "        b -= alpha * grad_b                             # Your Code Here\n",
        "\n",
        "        # Compute cost\n",
        "        cost = costfunction_logreg(X, y, w, b)          # Your Code Here\n",
        "\n",
        "        # Store cost and parameters\n",
        "        cost_history.append(cost)\n",
        "        params_history.append((w.copy(), b))\n",
        "\n",
        "        # Optionally print cost and parameters\n",
        "        if show_cost and (i % 100 == 0 or i == n_iter - 1):\n",
        "            print(f\"Iteration {i}: Cost = {cost:.6f}\")\n",
        "\n",
        "        if show_params and (i % 100 == 0 or i == n_iter - 1):\n",
        "            print(f\"Iteration {i}: w = {w}, b = {b:.6f}\")\n",
        "\n",
        "    return w, b, cost_history, params_history\n"
      ],
      "metadata": {
        "id": "HX6cV1Ybkx69"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[0.1, 0.2], [-0.1, 0.1]])\n",
        "y = np.array([1, 0])\n",
        "w = np.zeros(X.shape[1])\n",
        "b = 0.0\n",
        "alpha = 0.1\n",
        "n_iter = 100000\n",
        "\n",
        "w_out, b_out, cost_history, params_history = gradient_descent(\n",
        "    X, y, w, b, alpha, n_iter, show_cost=True, show_params=False\n",
        ")\n",
        "\n",
        "print(\"\\nFinal parameters:\")\n",
        "print(f\"w: {w_out}, b: {b_out}\")\n",
        "print(f\"Final cost: {cost_history[-1]:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPpArmtok0jG",
        "outputId": "1793634c-d370-4fe1-b427-47ba00f38875"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Cost = 0.692835\n",
            "Iteration 100: Cost = 0.662662\n",
            "Iteration 200: Cost = 0.634332\n",
            "Iteration 300: Cost = 0.607704\n",
            "Iteration 400: Cost = 0.582671\n",
            "Iteration 500: Cost = 0.559128\n",
            "Iteration 600: Cost = 0.536977\n",
            "Iteration 700: Cost = 0.516126\n",
            "Iteration 800: Cost = 0.496487\n",
            "Iteration 900: Cost = 0.477978\n",
            "Iteration 1000: Cost = 0.460524\n",
            "Iteration 1100: Cost = 0.444052\n",
            "Iteration 1200: Cost = 0.428497\n",
            "Iteration 1300: Cost = 0.413797\n",
            "Iteration 1400: Cost = 0.399895\n",
            "Iteration 1500: Cost = 0.386736\n",
            "Iteration 1600: Cost = 0.374272\n",
            "Iteration 1700: Cost = 0.362457\n",
            "Iteration 1800: Cost = 0.351248\n",
            "Iteration 1900: Cost = 0.340607\n",
            "Iteration 2000: Cost = 0.330495\n",
            "Iteration 2100: Cost = 0.320880\n",
            "Iteration 2200: Cost = 0.311730\n",
            "Iteration 2300: Cost = 0.303016\n",
            "Iteration 2400: Cost = 0.294710\n",
            "Iteration 2500: Cost = 0.286789\n",
            "Iteration 2600: Cost = 0.279228\n",
            "Iteration 2700: Cost = 0.272007\n",
            "Iteration 2800: Cost = 0.265104\n",
            "Iteration 2900: Cost = 0.258502\n",
            "Iteration 3000: Cost = 0.252182\n",
            "Iteration 3100: Cost = 0.246129\n",
            "Iteration 3200: Cost = 0.240328\n",
            "Iteration 3300: Cost = 0.234764\n",
            "Iteration 3400: Cost = 0.229425\n",
            "Iteration 3500: Cost = 0.224299\n",
            "Iteration 3600: Cost = 0.219373\n",
            "Iteration 3700: Cost = 0.214637\n",
            "Iteration 3800: Cost = 0.210081\n",
            "Iteration 3900: Cost = 0.205697\n",
            "Iteration 4000: Cost = 0.201474\n",
            "Iteration 4100: Cost = 0.197406\n",
            "Iteration 4200: Cost = 0.193484\n",
            "Iteration 4300: Cost = 0.189700\n",
            "Iteration 4400: Cost = 0.186049\n",
            "Iteration 4500: Cost = 0.182524\n",
            "Iteration 4600: Cost = 0.179119\n",
            "Iteration 4700: Cost = 0.175828\n",
            "Iteration 4800: Cost = 0.172646\n",
            "Iteration 4900: Cost = 0.169568\n",
            "Iteration 5000: Cost = 0.166589\n",
            "Iteration 5100: Cost = 0.163705\n",
            "Iteration 5200: Cost = 0.160912\n",
            "Iteration 5300: Cost = 0.158205\n",
            "Iteration 5400: Cost = 0.155581\n",
            "Iteration 5500: Cost = 0.153036\n",
            "Iteration 5600: Cost = 0.150568\n",
            "Iteration 5700: Cost = 0.148172\n",
            "Iteration 5800: Cost = 0.145846\n",
            "Iteration 5900: Cost = 0.143586\n",
            "Iteration 6000: Cost = 0.141392\n",
            "Iteration 6100: Cost = 0.139258\n",
            "Iteration 6200: Cost = 0.137184\n",
            "Iteration 6300: Cost = 0.135167\n",
            "Iteration 6400: Cost = 0.133205\n",
            "Iteration 6500: Cost = 0.131295\n",
            "Iteration 6600: Cost = 0.129436\n",
            "Iteration 6700: Cost = 0.127625\n",
            "Iteration 6800: Cost = 0.125862\n",
            "Iteration 6900: Cost = 0.124143\n",
            "Iteration 7000: Cost = 0.122469\n",
            "Iteration 7100: Cost = 0.120836\n",
            "Iteration 7200: Cost = 0.119243\n",
            "Iteration 7300: Cost = 0.117690\n",
            "Iteration 7400: Cost = 0.116175\n",
            "Iteration 7500: Cost = 0.114695\n",
            "Iteration 7600: Cost = 0.113251\n",
            "Iteration 7700: Cost = 0.111841\n",
            "Iteration 7800: Cost = 0.110464\n",
            "Iteration 7900: Cost = 0.109118\n",
            "Iteration 8000: Cost = 0.107804\n",
            "Iteration 8100: Cost = 0.106518\n",
            "Iteration 8200: Cost = 0.105262\n",
            "Iteration 8300: Cost = 0.104033\n",
            "Iteration 8400: Cost = 0.102832\n",
            "Iteration 8500: Cost = 0.101656\n",
            "Iteration 8600: Cost = 0.100506\n",
            "Iteration 8700: Cost = 0.099380\n",
            "Iteration 8800: Cost = 0.098278\n",
            "Iteration 8900: Cost = 0.097199\n",
            "Iteration 9000: Cost = 0.096142\n",
            "Iteration 9100: Cost = 0.095107\n",
            "Iteration 9200: Cost = 0.094093\n",
            "Iteration 9300: Cost = 0.093100\n",
            "Iteration 9400: Cost = 0.092126\n",
            "Iteration 9500: Cost = 0.091172\n",
            "Iteration 9600: Cost = 0.090236\n",
            "Iteration 9700: Cost = 0.089318\n",
            "Iteration 9800: Cost = 0.088418\n",
            "Iteration 9900: Cost = 0.087536\n",
            "Iteration 10000: Cost = 0.086670\n",
            "Iteration 10100: Cost = 0.085820\n",
            "Iteration 10200: Cost = 0.084986\n",
            "Iteration 10300: Cost = 0.084168\n",
            "Iteration 10400: Cost = 0.083364\n",
            "Iteration 10500: Cost = 0.082575\n",
            "Iteration 10600: Cost = 0.081800\n",
            "Iteration 10700: Cost = 0.081039\n",
            "Iteration 10800: Cost = 0.080292\n",
            "Iteration 10900: Cost = 0.079558\n",
            "Iteration 11000: Cost = 0.078836\n",
            "Iteration 11100: Cost = 0.078127\n",
            "Iteration 11200: Cost = 0.077430\n",
            "Iteration 11300: Cost = 0.076745\n",
            "Iteration 11400: Cost = 0.076072\n",
            "Iteration 11500: Cost = 0.075409\n",
            "Iteration 11600: Cost = 0.074758\n",
            "Iteration 11700: Cost = 0.074118\n",
            "Iteration 11800: Cost = 0.073488\n",
            "Iteration 11900: Cost = 0.072868\n",
            "Iteration 12000: Cost = 0.072259\n",
            "Iteration 12100: Cost = 0.071659\n",
            "Iteration 12200: Cost = 0.071068\n",
            "Iteration 12300: Cost = 0.070487\n",
            "Iteration 12400: Cost = 0.069915\n",
            "Iteration 12500: Cost = 0.069352\n",
            "Iteration 12600: Cost = 0.068798\n",
            "Iteration 12700: Cost = 0.068252\n",
            "Iteration 12800: Cost = 0.067714\n",
            "Iteration 12900: Cost = 0.067185\n",
            "Iteration 13000: Cost = 0.066663\n",
            "Iteration 13100: Cost = 0.066149\n",
            "Iteration 13200: Cost = 0.065643\n",
            "Iteration 13300: Cost = 0.065145\n",
            "Iteration 13400: Cost = 0.064653\n",
            "Iteration 13500: Cost = 0.064169\n",
            "Iteration 13600: Cost = 0.063692\n",
            "Iteration 13700: Cost = 0.063221\n",
            "Iteration 13800: Cost = 0.062757\n",
            "Iteration 13900: Cost = 0.062300\n",
            "Iteration 14000: Cost = 0.061849\n",
            "Iteration 14100: Cost = 0.061405\n",
            "Iteration 14200: Cost = 0.060966\n",
            "Iteration 14300: Cost = 0.060534\n",
            "Iteration 14400: Cost = 0.060108\n",
            "Iteration 14500: Cost = 0.059687\n",
            "Iteration 14600: Cost = 0.059272\n",
            "Iteration 14700: Cost = 0.058862\n",
            "Iteration 14800: Cost = 0.058459\n",
            "Iteration 14900: Cost = 0.058060\n",
            "Iteration 15000: Cost = 0.057667\n",
            "Iteration 15100: Cost = 0.057278\n",
            "Iteration 15200: Cost = 0.056895\n",
            "Iteration 15300: Cost = 0.056517\n",
            "Iteration 15400: Cost = 0.056144\n",
            "Iteration 15500: Cost = 0.055775\n",
            "Iteration 15600: Cost = 0.055411\n",
            "Iteration 15700: Cost = 0.055052\n",
            "Iteration 15800: Cost = 0.054697\n",
            "Iteration 15900: Cost = 0.054346\n",
            "Iteration 16000: Cost = 0.054000\n",
            "Iteration 16100: Cost = 0.053659\n",
            "Iteration 16200: Cost = 0.053321\n",
            "Iteration 16300: Cost = 0.052987\n",
            "Iteration 16400: Cost = 0.052658\n",
            "Iteration 16500: Cost = 0.052333\n",
            "Iteration 16600: Cost = 0.052011\n",
            "Iteration 16700: Cost = 0.051693\n",
            "Iteration 16800: Cost = 0.051379\n",
            "Iteration 16900: Cost = 0.051069\n",
            "Iteration 17000: Cost = 0.050762\n",
            "Iteration 17100: Cost = 0.050459\n",
            "Iteration 17200: Cost = 0.050159\n",
            "Iteration 17300: Cost = 0.049863\n",
            "Iteration 17400: Cost = 0.049571\n",
            "Iteration 17500: Cost = 0.049281\n",
            "Iteration 17600: Cost = 0.048995\n",
            "Iteration 17700: Cost = 0.048712\n",
            "Iteration 17800: Cost = 0.048432\n",
            "Iteration 17900: Cost = 0.048156\n",
            "Iteration 18000: Cost = 0.047882\n",
            "Iteration 18100: Cost = 0.047612\n",
            "Iteration 18200: Cost = 0.047344\n",
            "Iteration 18300: Cost = 0.047079\n",
            "Iteration 18400: Cost = 0.046818\n",
            "Iteration 18500: Cost = 0.046559\n",
            "Iteration 18600: Cost = 0.046302\n",
            "Iteration 18700: Cost = 0.046049\n",
            "Iteration 18800: Cost = 0.045798\n",
            "Iteration 18900: Cost = 0.045550\n",
            "Iteration 19000: Cost = 0.045305\n",
            "Iteration 19100: Cost = 0.045062\n",
            "Iteration 19200: Cost = 0.044822\n",
            "Iteration 19300: Cost = 0.044584\n",
            "Iteration 19400: Cost = 0.044348\n",
            "Iteration 19500: Cost = 0.044115\n",
            "Iteration 19600: Cost = 0.043885\n",
            "Iteration 19700: Cost = 0.043656\n",
            "Iteration 19800: Cost = 0.043430\n",
            "Iteration 19900: Cost = 0.043207\n",
            "Iteration 20000: Cost = 0.042985\n",
            "Iteration 20100: Cost = 0.042766\n",
            "Iteration 20200: Cost = 0.042549\n",
            "Iteration 20300: Cost = 0.042334\n",
            "Iteration 20400: Cost = 0.042121\n",
            "Iteration 20500: Cost = 0.041911\n",
            "Iteration 20600: Cost = 0.041702\n",
            "Iteration 20700: Cost = 0.041495\n",
            "Iteration 20800: Cost = 0.041291\n",
            "Iteration 20900: Cost = 0.041088\n",
            "Iteration 21000: Cost = 0.040888\n",
            "Iteration 21100: Cost = 0.040689\n",
            "Iteration 21200: Cost = 0.040492\n",
            "Iteration 21300: Cost = 0.040297\n",
            "Iteration 21400: Cost = 0.040104\n",
            "Iteration 21500: Cost = 0.039912\n",
            "Iteration 21600: Cost = 0.039722\n",
            "Iteration 21700: Cost = 0.039535\n",
            "Iteration 21800: Cost = 0.039348\n",
            "Iteration 21900: Cost = 0.039164\n",
            "Iteration 22000: Cost = 0.038981\n",
            "Iteration 22100: Cost = 0.038800\n",
            "Iteration 22200: Cost = 0.038621\n",
            "Iteration 22300: Cost = 0.038443\n",
            "Iteration 22400: Cost = 0.038267\n",
            "Iteration 22500: Cost = 0.038092\n",
            "Iteration 22600: Cost = 0.037919\n",
            "Iteration 22700: Cost = 0.037748\n",
            "Iteration 22800: Cost = 0.037577\n",
            "Iteration 22900: Cost = 0.037409\n",
            "Iteration 23000: Cost = 0.037242\n",
            "Iteration 23100: Cost = 0.037076\n",
            "Iteration 23200: Cost = 0.036912\n",
            "Iteration 23300: Cost = 0.036749\n",
            "Iteration 23400: Cost = 0.036588\n",
            "Iteration 23500: Cost = 0.036428\n",
            "Iteration 23600: Cost = 0.036270\n",
            "Iteration 23700: Cost = 0.036112\n",
            "Iteration 23800: Cost = 0.035956\n",
            "Iteration 23900: Cost = 0.035802\n",
            "Iteration 24000: Cost = 0.035649\n",
            "Iteration 24100: Cost = 0.035497\n",
            "Iteration 24200: Cost = 0.035346\n",
            "Iteration 24300: Cost = 0.035196\n",
            "Iteration 24400: Cost = 0.035048\n",
            "Iteration 24500: Cost = 0.034901\n",
            "Iteration 24600: Cost = 0.034755\n",
            "Iteration 24700: Cost = 0.034611\n",
            "Iteration 24800: Cost = 0.034467\n",
            "Iteration 24900: Cost = 0.034325\n",
            "Iteration 25000: Cost = 0.034184\n",
            "Iteration 25100: Cost = 0.034044\n",
            "Iteration 25200: Cost = 0.033905\n",
            "Iteration 25300: Cost = 0.033767\n",
            "Iteration 25400: Cost = 0.033631\n",
            "Iteration 25500: Cost = 0.033495\n",
            "Iteration 25600: Cost = 0.033361\n",
            "Iteration 25700: Cost = 0.033227\n",
            "Iteration 25800: Cost = 0.033095\n",
            "Iteration 25900: Cost = 0.032963\n",
            "Iteration 26000: Cost = 0.032833\n",
            "Iteration 26100: Cost = 0.032704\n",
            "Iteration 26200: Cost = 0.032575\n",
            "Iteration 26300: Cost = 0.032448\n",
            "Iteration 26400: Cost = 0.032322\n",
            "Iteration 26500: Cost = 0.032196\n",
            "Iteration 26600: Cost = 0.032072\n",
            "Iteration 26700: Cost = 0.031948\n",
            "Iteration 26800: Cost = 0.031826\n",
            "Iteration 26900: Cost = 0.031704\n",
            "Iteration 27000: Cost = 0.031583\n",
            "Iteration 27100: Cost = 0.031463\n",
            "Iteration 27200: Cost = 0.031344\n",
            "Iteration 27300: Cost = 0.031226\n",
            "Iteration 27400: Cost = 0.031109\n",
            "Iteration 27500: Cost = 0.030993\n",
            "Iteration 27600: Cost = 0.030877\n",
            "Iteration 27700: Cost = 0.030763\n",
            "Iteration 27800: Cost = 0.030649\n",
            "Iteration 27900: Cost = 0.030536\n",
            "Iteration 28000: Cost = 0.030424\n",
            "Iteration 28100: Cost = 0.030312\n",
            "Iteration 28200: Cost = 0.030202\n",
            "Iteration 28300: Cost = 0.030092\n",
            "Iteration 28400: Cost = 0.029983\n",
            "Iteration 28500: Cost = 0.029875\n",
            "Iteration 28600: Cost = 0.029768\n",
            "Iteration 28700: Cost = 0.029661\n",
            "Iteration 28800: Cost = 0.029555\n",
            "Iteration 28900: Cost = 0.029450\n",
            "Iteration 29000: Cost = 0.029345\n",
            "Iteration 29100: Cost = 0.029242\n",
            "Iteration 29200: Cost = 0.029139\n",
            "Iteration 29300: Cost = 0.029036\n",
            "Iteration 29400: Cost = 0.028935\n",
            "Iteration 29500: Cost = 0.028834\n",
            "Iteration 29600: Cost = 0.028734\n",
            "Iteration 29700: Cost = 0.028634\n",
            "Iteration 29800: Cost = 0.028535\n",
            "Iteration 29900: Cost = 0.028437\n",
            "Iteration 30000: Cost = 0.028340\n",
            "Iteration 30100: Cost = 0.028243\n",
            "Iteration 30200: Cost = 0.028147\n",
            "Iteration 30300: Cost = 0.028051\n",
            "Iteration 30400: Cost = 0.027956\n",
            "Iteration 30500: Cost = 0.027862\n",
            "Iteration 30600: Cost = 0.027768\n",
            "Iteration 30700: Cost = 0.027675\n",
            "Iteration 30800: Cost = 0.027583\n",
            "Iteration 30900: Cost = 0.027491\n",
            "Iteration 31000: Cost = 0.027400\n",
            "Iteration 31100: Cost = 0.027309\n",
            "Iteration 31200: Cost = 0.027219\n",
            "Iteration 31300: Cost = 0.027130\n",
            "Iteration 31400: Cost = 0.027041\n",
            "Iteration 31500: Cost = 0.026953\n",
            "Iteration 31600: Cost = 0.026865\n",
            "Iteration 31700: Cost = 0.026778\n",
            "Iteration 31800: Cost = 0.026691\n",
            "Iteration 31900: Cost = 0.026605\n",
            "Iteration 32000: Cost = 0.026519\n",
            "Iteration 32100: Cost = 0.026435\n",
            "Iteration 32200: Cost = 0.026350\n",
            "Iteration 32300: Cost = 0.026266\n",
            "Iteration 32400: Cost = 0.026183\n",
            "Iteration 32500: Cost = 0.026100\n",
            "Iteration 32600: Cost = 0.026018\n",
            "Iteration 32700: Cost = 0.025936\n",
            "Iteration 32800: Cost = 0.025854\n",
            "Iteration 32900: Cost = 0.025774\n",
            "Iteration 33000: Cost = 0.025693\n",
            "Iteration 33100: Cost = 0.025613\n",
            "Iteration 33200: Cost = 0.025534\n",
            "Iteration 33300: Cost = 0.025455\n",
            "Iteration 33400: Cost = 0.025377\n",
            "Iteration 33500: Cost = 0.025299\n",
            "Iteration 33600: Cost = 0.025222\n",
            "Iteration 33700: Cost = 0.025145\n",
            "Iteration 33800: Cost = 0.025068\n",
            "Iteration 33900: Cost = 0.024992\n",
            "Iteration 34000: Cost = 0.024916\n",
            "Iteration 34100: Cost = 0.024841\n",
            "Iteration 34200: Cost = 0.024767\n",
            "Iteration 34300: Cost = 0.024692\n",
            "Iteration 34400: Cost = 0.024619\n",
            "Iteration 34500: Cost = 0.024545\n",
            "Iteration 34600: Cost = 0.024472\n",
            "Iteration 34700: Cost = 0.024400\n",
            "Iteration 34800: Cost = 0.024328\n",
            "Iteration 34900: Cost = 0.024256\n",
            "Iteration 35000: Cost = 0.024185\n",
            "Iteration 35100: Cost = 0.024114\n",
            "Iteration 35200: Cost = 0.024043\n",
            "Iteration 35300: Cost = 0.023973\n",
            "Iteration 35400: Cost = 0.023904\n",
            "Iteration 35500: Cost = 0.023834\n",
            "Iteration 35600: Cost = 0.023766\n",
            "Iteration 35700: Cost = 0.023697\n",
            "Iteration 35800: Cost = 0.023629\n",
            "Iteration 35900: Cost = 0.023561\n",
            "Iteration 36000: Cost = 0.023494\n",
            "Iteration 36100: Cost = 0.023427\n",
            "Iteration 36200: Cost = 0.023361\n",
            "Iteration 36300: Cost = 0.023295\n",
            "Iteration 36400: Cost = 0.023229\n",
            "Iteration 36500: Cost = 0.023163\n",
            "Iteration 36600: Cost = 0.023098\n",
            "Iteration 36700: Cost = 0.023034\n",
            "Iteration 36800: Cost = 0.022969\n",
            "Iteration 36900: Cost = 0.022905\n",
            "Iteration 37000: Cost = 0.022842\n",
            "Iteration 37100: Cost = 0.022778\n",
            "Iteration 37200: Cost = 0.022715\n",
            "Iteration 37300: Cost = 0.022653\n",
            "Iteration 37400: Cost = 0.022590\n",
            "Iteration 37500: Cost = 0.022529\n",
            "Iteration 37600: Cost = 0.022467\n",
            "Iteration 37700: Cost = 0.022406\n",
            "Iteration 37800: Cost = 0.022345\n",
            "Iteration 37900: Cost = 0.022284\n",
            "Iteration 38000: Cost = 0.022224\n",
            "Iteration 38100: Cost = 0.022164\n",
            "Iteration 38200: Cost = 0.022104\n",
            "Iteration 38300: Cost = 0.022045\n",
            "Iteration 38400: Cost = 0.021986\n",
            "Iteration 38500: Cost = 0.021927\n",
            "Iteration 38600: Cost = 0.021869\n",
            "Iteration 38700: Cost = 0.021811\n",
            "Iteration 38800: Cost = 0.021753\n",
            "Iteration 38900: Cost = 0.021696\n",
            "Iteration 39000: Cost = 0.021638\n",
            "Iteration 39100: Cost = 0.021581\n",
            "Iteration 39200: Cost = 0.021525\n",
            "Iteration 39300: Cost = 0.021469\n",
            "Iteration 39400: Cost = 0.021413\n",
            "Iteration 39500: Cost = 0.021357\n",
            "Iteration 39600: Cost = 0.021301\n",
            "Iteration 39700: Cost = 0.021246\n",
            "Iteration 39800: Cost = 0.021191\n",
            "Iteration 39900: Cost = 0.021137\n",
            "Iteration 40000: Cost = 0.021083\n",
            "Iteration 40100: Cost = 0.021029\n",
            "Iteration 40200: Cost = 0.020975\n",
            "Iteration 40300: Cost = 0.020921\n",
            "Iteration 40400: Cost = 0.020868\n",
            "Iteration 40500: Cost = 0.020815\n",
            "Iteration 40600: Cost = 0.020762\n",
            "Iteration 40700: Cost = 0.020710\n",
            "Iteration 40800: Cost = 0.020658\n",
            "Iteration 40900: Cost = 0.020606\n",
            "Iteration 41000: Cost = 0.020554\n",
            "Iteration 41100: Cost = 0.020503\n",
            "Iteration 41200: Cost = 0.020452\n",
            "Iteration 41300: Cost = 0.020401\n",
            "Iteration 41400: Cost = 0.020350\n",
            "Iteration 41500: Cost = 0.020300\n",
            "Iteration 41600: Cost = 0.020250\n",
            "Iteration 41700: Cost = 0.020200\n",
            "Iteration 41800: Cost = 0.020150\n",
            "Iteration 41900: Cost = 0.020101\n",
            "Iteration 42000: Cost = 0.020052\n",
            "Iteration 42100: Cost = 0.020003\n",
            "Iteration 42200: Cost = 0.019954\n",
            "Iteration 42300: Cost = 0.019906\n",
            "Iteration 42400: Cost = 0.019857\n",
            "Iteration 42500: Cost = 0.019809\n",
            "Iteration 42600: Cost = 0.019762\n",
            "Iteration 42700: Cost = 0.019714\n",
            "Iteration 42800: Cost = 0.019667\n",
            "Iteration 42900: Cost = 0.019620\n",
            "Iteration 43000: Cost = 0.019573\n",
            "Iteration 43100: Cost = 0.019526\n",
            "Iteration 43200: Cost = 0.019480\n",
            "Iteration 43300: Cost = 0.019434\n",
            "Iteration 43400: Cost = 0.019388\n",
            "Iteration 43500: Cost = 0.019342\n",
            "Iteration 43600: Cost = 0.019296\n",
            "Iteration 43700: Cost = 0.019251\n",
            "Iteration 43800: Cost = 0.019206\n",
            "Iteration 43900: Cost = 0.019161\n",
            "Iteration 44000: Cost = 0.019116\n",
            "Iteration 44100: Cost = 0.019072\n",
            "Iteration 44200: Cost = 0.019027\n",
            "Iteration 44300: Cost = 0.018983\n",
            "Iteration 44400: Cost = 0.018939\n",
            "Iteration 44500: Cost = 0.018896\n",
            "Iteration 44600: Cost = 0.018852\n",
            "Iteration 44700: Cost = 0.018809\n",
            "Iteration 44800: Cost = 0.018766\n",
            "Iteration 44900: Cost = 0.018723\n",
            "Iteration 45000: Cost = 0.018680\n",
            "Iteration 45100: Cost = 0.018638\n",
            "Iteration 45200: Cost = 0.018595\n",
            "Iteration 45300: Cost = 0.018553\n",
            "Iteration 45400: Cost = 0.018511\n",
            "Iteration 45500: Cost = 0.018469\n",
            "Iteration 45600: Cost = 0.018428\n",
            "Iteration 45700: Cost = 0.018386\n",
            "Iteration 45800: Cost = 0.018345\n",
            "Iteration 45900: Cost = 0.018304\n",
            "Iteration 46000: Cost = 0.018263\n",
            "Iteration 46100: Cost = 0.018223\n",
            "Iteration 46200: Cost = 0.018182\n",
            "Iteration 46300: Cost = 0.018142\n",
            "Iteration 46400: Cost = 0.018102\n",
            "Iteration 46500: Cost = 0.018062\n",
            "Iteration 46600: Cost = 0.018022\n",
            "Iteration 46700: Cost = 0.017982\n",
            "Iteration 46800: Cost = 0.017943\n",
            "Iteration 46900: Cost = 0.017904\n",
            "Iteration 47000: Cost = 0.017864\n",
            "Iteration 47100: Cost = 0.017826\n",
            "Iteration 47200: Cost = 0.017787\n",
            "Iteration 47300: Cost = 0.017748\n",
            "Iteration 47400: Cost = 0.017710\n",
            "Iteration 47500: Cost = 0.017671\n",
            "Iteration 47600: Cost = 0.017633\n",
            "Iteration 47700: Cost = 0.017595\n",
            "Iteration 47800: Cost = 0.017558\n",
            "Iteration 47900: Cost = 0.017520\n",
            "Iteration 48000: Cost = 0.017483\n",
            "Iteration 48100: Cost = 0.017445\n",
            "Iteration 48200: Cost = 0.017408\n",
            "Iteration 48300: Cost = 0.017371\n",
            "Iteration 48400: Cost = 0.017334\n",
            "Iteration 48500: Cost = 0.017298\n",
            "Iteration 48600: Cost = 0.017261\n",
            "Iteration 48700: Cost = 0.017225\n",
            "Iteration 48800: Cost = 0.017189\n",
            "Iteration 48900: Cost = 0.017152\n",
            "Iteration 49000: Cost = 0.017117\n",
            "Iteration 49100: Cost = 0.017081\n",
            "Iteration 49200: Cost = 0.017045\n",
            "Iteration 49300: Cost = 0.017010\n",
            "Iteration 49400: Cost = 0.016974\n",
            "Iteration 49500: Cost = 0.016939\n",
            "Iteration 49600: Cost = 0.016904\n",
            "Iteration 49700: Cost = 0.016869\n",
            "Iteration 49800: Cost = 0.016834\n",
            "Iteration 49900: Cost = 0.016800\n",
            "Iteration 50000: Cost = 0.016765\n",
            "Iteration 50100: Cost = 0.016731\n",
            "Iteration 50200: Cost = 0.016697\n",
            "Iteration 50300: Cost = 0.016663\n",
            "Iteration 50400: Cost = 0.016629\n",
            "Iteration 50500: Cost = 0.016595\n",
            "Iteration 50600: Cost = 0.016561\n",
            "Iteration 50700: Cost = 0.016528\n",
            "Iteration 50800: Cost = 0.016495\n",
            "Iteration 50900: Cost = 0.016461\n",
            "Iteration 51000: Cost = 0.016428\n",
            "Iteration 51100: Cost = 0.016395\n",
            "Iteration 51200: Cost = 0.016362\n",
            "Iteration 51300: Cost = 0.016330\n",
            "Iteration 51400: Cost = 0.016297\n",
            "Iteration 51500: Cost = 0.016265\n",
            "Iteration 51600: Cost = 0.016232\n",
            "Iteration 51700: Cost = 0.016200\n",
            "Iteration 51800: Cost = 0.016168\n",
            "Iteration 51900: Cost = 0.016136\n",
            "Iteration 52000: Cost = 0.016104\n",
            "Iteration 52100: Cost = 0.016073\n",
            "Iteration 52200: Cost = 0.016041\n",
            "Iteration 52300: Cost = 0.016010\n",
            "Iteration 52400: Cost = 0.015978\n",
            "Iteration 52500: Cost = 0.015947\n",
            "Iteration 52600: Cost = 0.015916\n",
            "Iteration 52700: Cost = 0.015885\n",
            "Iteration 52800: Cost = 0.015854\n",
            "Iteration 52900: Cost = 0.015823\n",
            "Iteration 53000: Cost = 0.015793\n",
            "Iteration 53100: Cost = 0.015762\n",
            "Iteration 53200: Cost = 0.015732\n",
            "Iteration 53300: Cost = 0.015702\n",
            "Iteration 53400: Cost = 0.015672\n",
            "Iteration 53500: Cost = 0.015641\n",
            "Iteration 53600: Cost = 0.015612\n",
            "Iteration 53700: Cost = 0.015582\n",
            "Iteration 53800: Cost = 0.015552\n",
            "Iteration 53900: Cost = 0.015522\n",
            "Iteration 54000: Cost = 0.015493\n",
            "Iteration 54100: Cost = 0.015464\n",
            "Iteration 54200: Cost = 0.015434\n",
            "Iteration 54300: Cost = 0.015405\n",
            "Iteration 54400: Cost = 0.015376\n",
            "Iteration 54500: Cost = 0.015347\n",
            "Iteration 54600: Cost = 0.015318\n",
            "Iteration 54700: Cost = 0.015290\n",
            "Iteration 54800: Cost = 0.015261\n",
            "Iteration 54900: Cost = 0.015233\n",
            "Iteration 55000: Cost = 0.015204\n",
            "Iteration 55100: Cost = 0.015176\n",
            "Iteration 55200: Cost = 0.015148\n",
            "Iteration 55300: Cost = 0.015120\n",
            "Iteration 55400: Cost = 0.015092\n",
            "Iteration 55500: Cost = 0.015064\n",
            "Iteration 55600: Cost = 0.015036\n",
            "Iteration 55700: Cost = 0.015008\n",
            "Iteration 55800: Cost = 0.014981\n",
            "Iteration 55900: Cost = 0.014953\n",
            "Iteration 56000: Cost = 0.014926\n",
            "Iteration 56100: Cost = 0.014899\n",
            "Iteration 56200: Cost = 0.014872\n",
            "Iteration 56300: Cost = 0.014845\n",
            "Iteration 56400: Cost = 0.014818\n",
            "Iteration 56500: Cost = 0.014791\n",
            "Iteration 56600: Cost = 0.014764\n",
            "Iteration 56700: Cost = 0.014737\n",
            "Iteration 56800: Cost = 0.014711\n",
            "Iteration 56900: Cost = 0.014684\n",
            "Iteration 57000: Cost = 0.014658\n",
            "Iteration 57100: Cost = 0.014632\n",
            "Iteration 57200: Cost = 0.014605\n",
            "Iteration 57300: Cost = 0.014579\n",
            "Iteration 57400: Cost = 0.014553\n",
            "Iteration 57500: Cost = 0.014527\n",
            "Iteration 57600: Cost = 0.014501\n",
            "Iteration 57700: Cost = 0.014476\n",
            "Iteration 57800: Cost = 0.014450\n",
            "Iteration 57900: Cost = 0.014424\n",
            "Iteration 58000: Cost = 0.014399\n",
            "Iteration 58100: Cost = 0.014374\n",
            "Iteration 58200: Cost = 0.014348\n",
            "Iteration 58300: Cost = 0.014323\n",
            "Iteration 58400: Cost = 0.014298\n",
            "Iteration 58500: Cost = 0.014273\n",
            "Iteration 58600: Cost = 0.014248\n",
            "Iteration 58700: Cost = 0.014223\n",
            "Iteration 58800: Cost = 0.014198\n",
            "Iteration 58900: Cost = 0.014174\n",
            "Iteration 59000: Cost = 0.014149\n",
            "Iteration 59100: Cost = 0.014124\n",
            "Iteration 59200: Cost = 0.014100\n",
            "Iteration 59300: Cost = 0.014076\n",
            "Iteration 59400: Cost = 0.014051\n",
            "Iteration 59500: Cost = 0.014027\n",
            "Iteration 59600: Cost = 0.014003\n",
            "Iteration 59700: Cost = 0.013979\n",
            "Iteration 59800: Cost = 0.013955\n",
            "Iteration 59900: Cost = 0.013931\n",
            "Iteration 60000: Cost = 0.013907\n",
            "Iteration 60100: Cost = 0.013884\n",
            "Iteration 60200: Cost = 0.013860\n",
            "Iteration 60300: Cost = 0.013837\n",
            "Iteration 60400: Cost = 0.013813\n",
            "Iteration 60500: Cost = 0.013790\n",
            "Iteration 60600: Cost = 0.013766\n",
            "Iteration 60700: Cost = 0.013743\n",
            "Iteration 60800: Cost = 0.013720\n",
            "Iteration 60900: Cost = 0.013697\n",
            "Iteration 61000: Cost = 0.013674\n",
            "Iteration 61100: Cost = 0.013651\n",
            "Iteration 61200: Cost = 0.013628\n",
            "Iteration 61300: Cost = 0.013606\n",
            "Iteration 61400: Cost = 0.013583\n",
            "Iteration 61500: Cost = 0.013560\n",
            "Iteration 61600: Cost = 0.013538\n",
            "Iteration 61700: Cost = 0.013515\n",
            "Iteration 61800: Cost = 0.013493\n",
            "Iteration 61900: Cost = 0.013471\n",
            "Iteration 62000: Cost = 0.013448\n",
            "Iteration 62100: Cost = 0.013426\n",
            "Iteration 62200: Cost = 0.013404\n",
            "Iteration 62300: Cost = 0.013382\n",
            "Iteration 62400: Cost = 0.013360\n",
            "Iteration 62500: Cost = 0.013338\n",
            "Iteration 62600: Cost = 0.013316\n",
            "Iteration 62700: Cost = 0.013295\n",
            "Iteration 62800: Cost = 0.013273\n",
            "Iteration 62900: Cost = 0.013251\n",
            "Iteration 63000: Cost = 0.013230\n",
            "Iteration 63100: Cost = 0.013208\n",
            "Iteration 63200: Cost = 0.013187\n",
            "Iteration 63300: Cost = 0.013166\n",
            "Iteration 63400: Cost = 0.013144\n",
            "Iteration 63500: Cost = 0.013123\n",
            "Iteration 63600: Cost = 0.013102\n",
            "Iteration 63700: Cost = 0.013081\n",
            "Iteration 63800: Cost = 0.013060\n",
            "Iteration 63900: Cost = 0.013039\n",
            "Iteration 64000: Cost = 0.013018\n",
            "Iteration 64100: Cost = 0.012997\n",
            "Iteration 64200: Cost = 0.012977\n",
            "Iteration 64300: Cost = 0.012956\n",
            "Iteration 64400: Cost = 0.012935\n",
            "Iteration 64500: Cost = 0.012915\n",
            "Iteration 64600: Cost = 0.012895\n",
            "Iteration 64700: Cost = 0.012874\n",
            "Iteration 64800: Cost = 0.012854\n",
            "Iteration 64900: Cost = 0.012834\n",
            "Iteration 65000: Cost = 0.012813\n",
            "Iteration 65100: Cost = 0.012793\n",
            "Iteration 65200: Cost = 0.012773\n",
            "Iteration 65300: Cost = 0.012753\n",
            "Iteration 65400: Cost = 0.012733\n",
            "Iteration 65500: Cost = 0.012713\n",
            "Iteration 65600: Cost = 0.012693\n",
            "Iteration 65700: Cost = 0.012674\n",
            "Iteration 65800: Cost = 0.012654\n",
            "Iteration 65900: Cost = 0.012634\n",
            "Iteration 66000: Cost = 0.012615\n",
            "Iteration 66100: Cost = 0.012595\n",
            "Iteration 66200: Cost = 0.012576\n",
            "Iteration 66300: Cost = 0.012556\n",
            "Iteration 66400: Cost = 0.012537\n",
            "Iteration 66500: Cost = 0.012518\n",
            "Iteration 66600: Cost = 0.012498\n",
            "Iteration 66700: Cost = 0.012479\n",
            "Iteration 66800: Cost = 0.012460\n",
            "Iteration 66900: Cost = 0.012441\n",
            "Iteration 67000: Cost = 0.012422\n",
            "Iteration 67100: Cost = 0.012403\n",
            "Iteration 67200: Cost = 0.012384\n",
            "Iteration 67300: Cost = 0.012365\n",
            "Iteration 67400: Cost = 0.012347\n",
            "Iteration 67500: Cost = 0.012328\n",
            "Iteration 67600: Cost = 0.012309\n",
            "Iteration 67700: Cost = 0.012291\n",
            "Iteration 67800: Cost = 0.012272\n",
            "Iteration 67900: Cost = 0.012254\n",
            "Iteration 68000: Cost = 0.012235\n",
            "Iteration 68100: Cost = 0.012217\n",
            "Iteration 68200: Cost = 0.012199\n",
            "Iteration 68300: Cost = 0.012180\n",
            "Iteration 68400: Cost = 0.012162\n",
            "Iteration 68500: Cost = 0.012144\n",
            "Iteration 68600: Cost = 0.012126\n",
            "Iteration 68700: Cost = 0.012108\n",
            "Iteration 68800: Cost = 0.012090\n",
            "Iteration 68900: Cost = 0.012072\n",
            "Iteration 69000: Cost = 0.012054\n",
            "Iteration 69100: Cost = 0.012036\n",
            "Iteration 69200: Cost = 0.012018\n",
            "Iteration 69300: Cost = 0.012001\n",
            "Iteration 69400: Cost = 0.011983\n",
            "Iteration 69500: Cost = 0.011965\n",
            "Iteration 69600: Cost = 0.011948\n",
            "Iteration 69700: Cost = 0.011930\n",
            "Iteration 69800: Cost = 0.011913\n",
            "Iteration 69900: Cost = 0.011895\n",
            "Iteration 70000: Cost = 0.011878\n",
            "Iteration 70100: Cost = 0.011861\n",
            "Iteration 70200: Cost = 0.011843\n",
            "Iteration 70300: Cost = 0.011826\n",
            "Iteration 70400: Cost = 0.011809\n",
            "Iteration 70500: Cost = 0.011792\n",
            "Iteration 70600: Cost = 0.011775\n",
            "Iteration 70700: Cost = 0.011758\n",
            "Iteration 70800: Cost = 0.011741\n",
            "Iteration 70900: Cost = 0.011724\n",
            "Iteration 71000: Cost = 0.011707\n",
            "Iteration 71100: Cost = 0.011690\n",
            "Iteration 71200: Cost = 0.011673\n",
            "Iteration 71300: Cost = 0.011656\n",
            "Iteration 71400: Cost = 0.011640\n",
            "Iteration 71500: Cost = 0.011623\n",
            "Iteration 71600: Cost = 0.011607\n",
            "Iteration 71700: Cost = 0.011590\n",
            "Iteration 71800: Cost = 0.011574\n",
            "Iteration 71900: Cost = 0.011557\n",
            "Iteration 72000: Cost = 0.011541\n",
            "Iteration 72100: Cost = 0.011524\n",
            "Iteration 72200: Cost = 0.011508\n",
            "Iteration 72300: Cost = 0.011492\n",
            "Iteration 72400: Cost = 0.011475\n",
            "Iteration 72500: Cost = 0.011459\n",
            "Iteration 72600: Cost = 0.011443\n",
            "Iteration 72700: Cost = 0.011427\n",
            "Iteration 72800: Cost = 0.011411\n",
            "Iteration 72900: Cost = 0.011395\n",
            "Iteration 73000: Cost = 0.011379\n",
            "Iteration 73100: Cost = 0.011363\n",
            "Iteration 73200: Cost = 0.011347\n",
            "Iteration 73300: Cost = 0.011331\n",
            "Iteration 73400: Cost = 0.011316\n",
            "Iteration 73500: Cost = 0.011300\n",
            "Iteration 73600: Cost = 0.011284\n",
            "Iteration 73700: Cost = 0.011269\n",
            "Iteration 73800: Cost = 0.011253\n",
            "Iteration 73900: Cost = 0.011237\n",
            "Iteration 74000: Cost = 0.011222\n",
            "Iteration 74100: Cost = 0.011206\n",
            "Iteration 74200: Cost = 0.011191\n",
            "Iteration 74300: Cost = 0.011176\n",
            "Iteration 74400: Cost = 0.011160\n",
            "Iteration 74500: Cost = 0.011145\n",
            "Iteration 74600: Cost = 0.011130\n",
            "Iteration 74700: Cost = 0.011114\n",
            "Iteration 74800: Cost = 0.011099\n",
            "Iteration 74900: Cost = 0.011084\n",
            "Iteration 75000: Cost = 0.011069\n",
            "Iteration 75100: Cost = 0.011054\n",
            "Iteration 75200: Cost = 0.011039\n",
            "Iteration 75300: Cost = 0.011024\n",
            "Iteration 75400: Cost = 0.011009\n",
            "Iteration 75500: Cost = 0.010994\n",
            "Iteration 75600: Cost = 0.010979\n",
            "Iteration 75700: Cost = 0.010964\n",
            "Iteration 75800: Cost = 0.010950\n",
            "Iteration 75900: Cost = 0.010935\n",
            "Iteration 76000: Cost = 0.010920\n",
            "Iteration 76100: Cost = 0.010906\n",
            "Iteration 76200: Cost = 0.010891\n",
            "Iteration 76300: Cost = 0.010876\n",
            "Iteration 76400: Cost = 0.010862\n",
            "Iteration 76500: Cost = 0.010847\n",
            "Iteration 76600: Cost = 0.010833\n",
            "Iteration 76700: Cost = 0.010818\n",
            "Iteration 76800: Cost = 0.010804\n",
            "Iteration 76900: Cost = 0.010790\n",
            "Iteration 77000: Cost = 0.010775\n",
            "Iteration 77100: Cost = 0.010761\n",
            "Iteration 77200: Cost = 0.010747\n",
            "Iteration 77300: Cost = 0.010733\n",
            "Iteration 77400: Cost = 0.010719\n",
            "Iteration 77500: Cost = 0.010704\n",
            "Iteration 77600: Cost = 0.010690\n",
            "Iteration 77700: Cost = 0.010676\n",
            "Iteration 77800: Cost = 0.010662\n",
            "Iteration 77900: Cost = 0.010648\n",
            "Iteration 78000: Cost = 0.010634\n",
            "Iteration 78100: Cost = 0.010620\n",
            "Iteration 78200: Cost = 0.010607\n",
            "Iteration 78300: Cost = 0.010593\n",
            "Iteration 78400: Cost = 0.010579\n",
            "Iteration 78500: Cost = 0.010565\n",
            "Iteration 78600: Cost = 0.010551\n",
            "Iteration 78700: Cost = 0.010538\n",
            "Iteration 78800: Cost = 0.010524\n",
            "Iteration 78900: Cost = 0.010510\n",
            "Iteration 79000: Cost = 0.010497\n",
            "Iteration 79100: Cost = 0.010483\n",
            "Iteration 79200: Cost = 0.010470\n",
            "Iteration 79300: Cost = 0.010456\n",
            "Iteration 79400: Cost = 0.010443\n",
            "Iteration 79500: Cost = 0.010429\n",
            "Iteration 79600: Cost = 0.010416\n",
            "Iteration 79700: Cost = 0.010403\n",
            "Iteration 79800: Cost = 0.010389\n",
            "Iteration 79900: Cost = 0.010376\n",
            "Iteration 80000: Cost = 0.010363\n",
            "Iteration 80100: Cost = 0.010350\n",
            "Iteration 80200: Cost = 0.010337\n",
            "Iteration 80300: Cost = 0.010323\n",
            "Iteration 80400: Cost = 0.010310\n",
            "Iteration 80500: Cost = 0.010297\n",
            "Iteration 80600: Cost = 0.010284\n",
            "Iteration 80700: Cost = 0.010271\n",
            "Iteration 80800: Cost = 0.010258\n",
            "Iteration 80900: Cost = 0.010245\n",
            "Iteration 81000: Cost = 0.010232\n",
            "Iteration 81100: Cost = 0.010219\n",
            "Iteration 81200: Cost = 0.010207\n",
            "Iteration 81300: Cost = 0.010194\n",
            "Iteration 81400: Cost = 0.010181\n",
            "Iteration 81500: Cost = 0.010168\n",
            "Iteration 81600: Cost = 0.010155\n",
            "Iteration 81700: Cost = 0.010143\n",
            "Iteration 81800: Cost = 0.010130\n",
            "Iteration 81900: Cost = 0.010117\n",
            "Iteration 82000: Cost = 0.010105\n",
            "Iteration 82100: Cost = 0.010092\n",
            "Iteration 82200: Cost = 0.010080\n",
            "Iteration 82300: Cost = 0.010067\n",
            "Iteration 82400: Cost = 0.010055\n",
            "Iteration 82500: Cost = 0.010042\n",
            "Iteration 82600: Cost = 0.010030\n",
            "Iteration 82700: Cost = 0.010018\n",
            "Iteration 82800: Cost = 0.010005\n",
            "Iteration 82900: Cost = 0.009993\n",
            "Iteration 83000: Cost = 0.009981\n",
            "Iteration 83100: Cost = 0.009968\n",
            "Iteration 83200: Cost = 0.009956\n",
            "Iteration 83300: Cost = 0.009944\n",
            "Iteration 83400: Cost = 0.009932\n",
            "Iteration 83500: Cost = 0.009920\n",
            "Iteration 83600: Cost = 0.009908\n",
            "Iteration 83700: Cost = 0.009895\n",
            "Iteration 83800: Cost = 0.009883\n",
            "Iteration 83900: Cost = 0.009871\n",
            "Iteration 84000: Cost = 0.009859\n",
            "Iteration 84100: Cost = 0.009847\n",
            "Iteration 84200: Cost = 0.009835\n",
            "Iteration 84300: Cost = 0.009824\n",
            "Iteration 84400: Cost = 0.009812\n",
            "Iteration 84500: Cost = 0.009800\n",
            "Iteration 84600: Cost = 0.009788\n",
            "Iteration 84700: Cost = 0.009776\n",
            "Iteration 84800: Cost = 0.009764\n",
            "Iteration 84900: Cost = 0.009753\n",
            "Iteration 85000: Cost = 0.009741\n",
            "Iteration 85100: Cost = 0.009729\n",
            "Iteration 85200: Cost = 0.009718\n",
            "Iteration 85300: Cost = 0.009706\n",
            "Iteration 85400: Cost = 0.009694\n",
            "Iteration 85500: Cost = 0.009683\n",
            "Iteration 85600: Cost = 0.009671\n",
            "Iteration 85700: Cost = 0.009660\n",
            "Iteration 85800: Cost = 0.009648\n",
            "Iteration 85900: Cost = 0.009637\n",
            "Iteration 86000: Cost = 0.009625\n",
            "Iteration 86100: Cost = 0.009614\n",
            "Iteration 86200: Cost = 0.009603\n",
            "Iteration 86300: Cost = 0.009591\n",
            "Iteration 86400: Cost = 0.009580\n",
            "Iteration 86500: Cost = 0.009569\n",
            "Iteration 86600: Cost = 0.009557\n",
            "Iteration 86700: Cost = 0.009546\n",
            "Iteration 86800: Cost = 0.009535\n",
            "Iteration 86900: Cost = 0.009524\n",
            "Iteration 87000: Cost = 0.009513\n",
            "Iteration 87100: Cost = 0.009501\n",
            "Iteration 87200: Cost = 0.009490\n",
            "Iteration 87300: Cost = 0.009479\n",
            "Iteration 87400: Cost = 0.009468\n",
            "Iteration 87500: Cost = 0.009457\n",
            "Iteration 87600: Cost = 0.009446\n",
            "Iteration 87700: Cost = 0.009435\n",
            "Iteration 87800: Cost = 0.009424\n",
            "Iteration 87900: Cost = 0.009413\n",
            "Iteration 88000: Cost = 0.009402\n",
            "Iteration 88100: Cost = 0.009391\n",
            "Iteration 88200: Cost = 0.009381\n",
            "Iteration 88300: Cost = 0.009370\n",
            "Iteration 88400: Cost = 0.009359\n",
            "Iteration 88500: Cost = 0.009348\n",
            "Iteration 88600: Cost = 0.009337\n",
            "Iteration 88700: Cost = 0.009327\n",
            "Iteration 88800: Cost = 0.009316\n",
            "Iteration 88900: Cost = 0.009305\n",
            "Iteration 89000: Cost = 0.009295\n",
            "Iteration 89100: Cost = 0.009284\n",
            "Iteration 89200: Cost = 0.009273\n",
            "Iteration 89300: Cost = 0.009263\n",
            "Iteration 89400: Cost = 0.009252\n",
            "Iteration 89500: Cost = 0.009242\n",
            "Iteration 89600: Cost = 0.009231\n",
            "Iteration 89700: Cost = 0.009221\n",
            "Iteration 89800: Cost = 0.009210\n",
            "Iteration 89900: Cost = 0.009200\n",
            "Iteration 90000: Cost = 0.009189\n",
            "Iteration 90100: Cost = 0.009179\n",
            "Iteration 90200: Cost = 0.009168\n",
            "Iteration 90300: Cost = 0.009158\n",
            "Iteration 90400: Cost = 0.009148\n",
            "Iteration 90500: Cost = 0.009138\n",
            "Iteration 90600: Cost = 0.009127\n",
            "Iteration 90700: Cost = 0.009117\n",
            "Iteration 90800: Cost = 0.009107\n",
            "Iteration 90900: Cost = 0.009096\n",
            "Iteration 91000: Cost = 0.009086\n",
            "Iteration 91100: Cost = 0.009076\n",
            "Iteration 91200: Cost = 0.009066\n",
            "Iteration 91300: Cost = 0.009056\n",
            "Iteration 91400: Cost = 0.009046\n",
            "Iteration 91500: Cost = 0.009036\n",
            "Iteration 91600: Cost = 0.009026\n",
            "Iteration 91700: Cost = 0.009016\n",
            "Iteration 91800: Cost = 0.009006\n",
            "Iteration 91900: Cost = 0.008996\n",
            "Iteration 92000: Cost = 0.008986\n",
            "Iteration 92100: Cost = 0.008976\n",
            "Iteration 92200: Cost = 0.008966\n",
            "Iteration 92300: Cost = 0.008956\n",
            "Iteration 92400: Cost = 0.008946\n",
            "Iteration 92500: Cost = 0.008936\n",
            "Iteration 92600: Cost = 0.008926\n",
            "Iteration 92700: Cost = 0.008916\n",
            "Iteration 92800: Cost = 0.008907\n",
            "Iteration 92900: Cost = 0.008897\n",
            "Iteration 93000: Cost = 0.008887\n",
            "Iteration 93100: Cost = 0.008877\n",
            "Iteration 93200: Cost = 0.008868\n",
            "Iteration 93300: Cost = 0.008858\n",
            "Iteration 93400: Cost = 0.008848\n",
            "Iteration 93500: Cost = 0.008839\n",
            "Iteration 93600: Cost = 0.008829\n",
            "Iteration 93700: Cost = 0.008819\n",
            "Iteration 93800: Cost = 0.008810\n",
            "Iteration 93900: Cost = 0.008800\n",
            "Iteration 94000: Cost = 0.008791\n",
            "Iteration 94100: Cost = 0.008781\n",
            "Iteration 94200: Cost = 0.008772\n",
            "Iteration 94300: Cost = 0.008762\n",
            "Iteration 94400: Cost = 0.008753\n",
            "Iteration 94500: Cost = 0.008743\n",
            "Iteration 94600: Cost = 0.008734\n",
            "Iteration 94700: Cost = 0.008725\n",
            "Iteration 94800: Cost = 0.008715\n",
            "Iteration 94900: Cost = 0.008706\n",
            "Iteration 95000: Cost = 0.008696\n",
            "Iteration 95100: Cost = 0.008687\n",
            "Iteration 95200: Cost = 0.008678\n",
            "Iteration 95300: Cost = 0.008669\n",
            "Iteration 95400: Cost = 0.008659\n",
            "Iteration 95500: Cost = 0.008650\n",
            "Iteration 95600: Cost = 0.008641\n",
            "Iteration 95700: Cost = 0.008632\n",
            "Iteration 95800: Cost = 0.008622\n",
            "Iteration 95900: Cost = 0.008613\n",
            "Iteration 96000: Cost = 0.008604\n",
            "Iteration 96100: Cost = 0.008595\n",
            "Iteration 96200: Cost = 0.008586\n",
            "Iteration 96300: Cost = 0.008577\n",
            "Iteration 96400: Cost = 0.008568\n",
            "Iteration 96500: Cost = 0.008559\n",
            "Iteration 96600: Cost = 0.008550\n",
            "Iteration 96700: Cost = 0.008541\n",
            "Iteration 96800: Cost = 0.008532\n",
            "Iteration 96900: Cost = 0.008523\n",
            "Iteration 97000: Cost = 0.008514\n",
            "Iteration 97100: Cost = 0.008505\n",
            "Iteration 97200: Cost = 0.008496\n",
            "Iteration 97300: Cost = 0.008487\n",
            "Iteration 97400: Cost = 0.008478\n",
            "Iteration 97500: Cost = 0.008469\n",
            "Iteration 97600: Cost = 0.008460\n",
            "Iteration 97700: Cost = 0.008452\n",
            "Iteration 97800: Cost = 0.008443\n",
            "Iteration 97900: Cost = 0.008434\n",
            "Iteration 98000: Cost = 0.008425\n",
            "Iteration 98100: Cost = 0.008416\n",
            "Iteration 98200: Cost = 0.008408\n",
            "Iteration 98300: Cost = 0.008399\n",
            "Iteration 98400: Cost = 0.008390\n",
            "Iteration 98500: Cost = 0.008382\n",
            "Iteration 98600: Cost = 0.008373\n",
            "Iteration 98700: Cost = 0.008364\n",
            "Iteration 98800: Cost = 0.008356\n",
            "Iteration 98900: Cost = 0.008347\n",
            "Iteration 99000: Cost = 0.008339\n",
            "Iteration 99100: Cost = 0.008330\n",
            "Iteration 99200: Cost = 0.008321\n",
            "Iteration 99300: Cost = 0.008313\n",
            "Iteration 99400: Cost = 0.008304\n",
            "Iteration 99500: Cost = 0.008296\n",
            "Iteration 99600: Cost = 0.008287\n",
            "Iteration 99700: Cost = 0.008279\n",
            "Iteration 99800: Cost = 0.008270\n",
            "Iteration 99900: Cost = 0.008262\n",
            "Iteration 99999: Cost = 0.008254\n",
            "\n",
            "Final parameters:\n",
            "w: [38.51304248 18.83386869], b: -2.8176836626325836\n",
            "Final cost: 0.008254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_gradient_descent():\n",
        "\n",
        "    X = np.array([[0.1, 0.2], [-0.1, 0.1]])\n",
        "    y = np.array([1, 0])\n",
        "    w = np.zeros(X.shape[1])\n",
        "    b = 0.0\n",
        "    alpha = 0.1\n",
        "    n_iter = 100\n",
        "\n",
        "    w_out, b_out, cost_history, _ = gradient_descent(\n",
        "        X, y, w, b, alpha, n_iter, show_cost=False, show_params=False\n",
        "    )\n",
        "\n",
        "    assert len(cost_history) == n_iter, \"Cost history length does not match the number of iterations\"\n",
        "    assert w_out.shape == w.shape, \"Shape of output weights does not match the initial weights\"\n",
        "    assert isinstance(b_out, float), \"Bias output is not a float\"\n",
        "    assert cost_history[-1] < cost_history[0], \"Cost did not decrease over iterations\"\n",
        "\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "test_gradient_descent()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNkvCFfBk0g2",
        "outputId": "75f4008e-6c69-4fc4-c117-4f8bc05b143a"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(X, w, b, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Predicts binary outcomes for given input features based on logistic regression parameters.\n",
        "    Arguments:\n",
        "    X (ndarray, shape (n,d)): Array of test independent variables (features) with n samples and d features.\n",
        "    w (ndarray, shape (d,)): Array of weights learned via gradient descent.\n",
        "    b (float): Bias learned via gradient descent.\n",
        "    threshold (float, optional): Classification threshold for predicting class labels. Default is 0.5.\n",
        "    Returns:\n",
        "    y_pred (ndarray, shape (n,)): Array of predicted dependent variable (binary class labels: 0 or 1).\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute the predicted probabilities using the logistic function\n",
        "    y_test_prob = logistic_function(np.dot(X, w) + b)   # Your Code Here\n",
        "\n",
        "    # Classify based on the threshold\n",
        "    y_pred = (y_test_prob >= threshold).astype(int)     # Your Code Here\n",
        "\n",
        "    return y_pred\n"
      ],
      "metadata": {
        "id": "zOscsGaVk841"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_prediction():\n",
        "\n",
        "    X_test = np.array([[0.5, 1.0], [1.5, -0.5], [-0.5, -1.0]])\n",
        "    w_test = np.array([1.0, -1.0])\n",
        "    b_test = 0.0\n",
        "    threshold = 0.5\n",
        "\n",
        "    expected_output = np.array([0, 1, 1])\n",
        "\n",
        "    y_pred = prediction(X_test, w_test, b_test, threshold)\n",
        "\n",
        "    assert np.array_equal(y_pred, expected_output), f\"Expected {expected_output}, but got {y_pred}\"\n",
        "\n",
        "    print(\"Test passed!\")\n",
        "\n",
        "test_prediction()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5RM2pksk-Sl",
        "outputId": "427ccfa7-3d68-454c-f473-235a63ed15ec"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_classification(y_true, y_pred):\n",
        "    # True Positives, True Negatives, False Positives, False Negatives\n",
        "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
        "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
        "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    confusion_matrix = np.array([\n",
        "        [TN, FP],\n",
        "        [FN, TP]\n",
        "    ])\n",
        "\n",
        "    # Precision, Recall, F1-score\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
        "    f1_score = (2 * precision * recall) / (precision + recall) \\\n",
        "               if (precision + recall) > 0 else 0.0\n",
        "\n",
        "    # Return results as a tuple\n",
        "    return confusion_matrix, precision, recall, f1_score\n"
      ],
      "metadata": {
        "id": "mbK7rJ6ylBvu"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = np.array([1, 0, 1, 1, 0])\n",
        "y_pred = np.array([1, 0, 0, 1, 1])\n",
        "\n",
        "confusion_matrix, precision, recall, f1_score = evaluate_classification(y_true, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBu1__UQlEpF",
        "outputId": "2c7d6285-7176-4b35-9be4-0157a1ae504b"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[1 1]\n",
            " [1 2]]\n",
            "Precision: 0.6666666666666666\n",
            "Recall: 0.6666666666666666\n",
            "F1 Score: 0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "columns = ['Pregnancies', 'Glucose', 'BloodPressure','SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "data_pima_diabetes = pd.read_csv(url, names=columns)"
      ],
      "metadata": {
        "id": "INgWGuNOlN_K"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data cleaning\n",
        "columns_to_clean = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
        "data_pima_diabetes[columns_to_clean] = data_pima_diabetes[columns_to_clean].replace(0, np.nan)\n",
        "data_pima_diabetes.fillna(data_pima_diabetes.median(), inplace=True)\n",
        "data_pima_diabetes.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrPxX7sUlv7i",
        "outputId": "964cf6ed-3856-4f1d-bb5b-5667eeaab0d0"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 768 entries, 0 to 767\n",
            "Data columns (total 9 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   Pregnancies               768 non-null    int64  \n",
            " 1   Glucose                   768 non-null    float64\n",
            " 2   BloodPressure             768 non-null    float64\n",
            " 3   SkinThickness             768 non-null    float64\n",
            " 4   Insulin                   768 non-null    float64\n",
            " 5   BMI                       768 non-null    float64\n",
            " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
            " 7   Age                       768 non-null    int64  \n",
            " 8   Outcome                   768 non-null    int64  \n",
            "dtypes: float64(6), int64(3)\n",
            "memory usage: 54.1 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "X = data_pima_diabetes.drop(columns=['Outcome']).values\n",
        "y = data_pima_diabetes['Outcome'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "bcLMd2M4m5qx"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize parameters\n",
        "w = np.zeros(X_train_scaled.shape[1])\n",
        "b = 0.0\n",
        "alpha = 0.1\n",
        "n_iter = 1000\n",
        "# Train model\n",
        "print(\"\\nTraining Logistic Regression Model:\")\n",
        "w, b, cost_history,params_history = gradient_descent(X_train_scaled, y_train, w, b, alpha, n_iter, show_cost=True, show_params=False)\n",
        "# Plot cost history\n",
        "plt.figure(figsize=(9, 6))\n",
        "plt.plot(cost_history)\n",
        "plt.xlabel(\"Iteration\", fontsize=14)\n",
        "plt.ylabel(\"Cost\", fontsize=14)\n",
        "plt.title(\"Cost vs Iteration\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        },
        "id": "wAkZBtYXm9K5",
        "outputId": "dc8d23f7-fca9-4576-d186-6f92ce043d5f"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Logistic Regression Model:\n",
            "Iteration 0: Cost = 0.676575\n",
            "Iteration 100: Cost = 0.465441\n",
            "Iteration 200: Cost = 0.455913\n",
            "Iteration 300: Cost = 0.453874\n",
            "Iteration 400: Cost = 0.453316\n",
            "Iteration 500: Cost = 0.453148\n",
            "Iteration 600: Cost = 0.453096\n",
            "Iteration 700: Cost = 0.453079\n",
            "Iteration 800: Cost = 0.453074\n",
            "Iteration 900: Cost = 0.453072\n",
            "Iteration 999: Cost = 0.453071\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 900x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test model\n",
        "y_train_pred = prediction(X_train_scaled, w, b)\n",
        "y_test_pred = prediction(X_test_scaled, w, b)\n",
        "# Evaluate train and test performance\n",
        "train_cost = costfunction_logreg(X_train_scaled, y_train, w, b)\n",
        "test_cost = costfunction_logreg(X_test_scaled, y_test, w, b)\n",
        "print(f\"\\nTrain Loss (Cost): {train_cost:.4f}\")\n",
        "print(f\"Test Loss (Cost): {test_cost:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAnQ4APwnTDQ",
        "outputId": "cfee6f7d-e255-49d5-a843-ad74f3f04483"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss (Cost): 0.4531\n",
            "Test Loss (Cost): 0.5146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy on test data\n",
        "test_accuracy = np.mean(y_test_pred == y_test) * 100\n",
        "print(f\"\\nTest Accuracy: {test_accuracy:.2f}%\")\n",
        "# Evaluation\n",
        "confusion_matrix, precision, recall, f1_score = evaluate_classification(y_test, y_test_pred)\n",
        "print(f\"\\nConfusion Matrix:\\n{confusion_matrix}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1_score:.2f}\")\n",
        "#Optional - Visualizing the COnfusion matrix\n",
        "# Visualizing Confusion Matrix\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "ax.imshow(confusion_matrix)\n",
        "ax.grid(False)\n",
        "ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
        "ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
        "ax.set_ylim(1.5, -0.5)\n",
        "for i in range(2):\n",
        "  for j in range(2):\n",
        "    ax.text(j, i, confusion_matrix[i, j], ha='center', va='center', color='white')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "bLZkXCpfnV7E",
        "outputId": "6359ba72-50c0-407f-e10a-ff496f08bcb2"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Accuracy: 70.78%\n",
            "\n",
            "Confusion Matrix:\n",
            "[[82 18]\n",
            " [27 27]]\n",
            "Precision: 0.60\n",
            "Recall: 0.50\n",
            "F1-Score: 0.55\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TnA2SmicnxBp"
      },
      "execution_count": 113,
      "outputs": []
    }
  ]
}